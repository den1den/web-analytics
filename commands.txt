## Do a run
cat tweets/test6.csv | venv/bin/python ./mapper.py | sort -k1,1 | venv/bin/python ./reducer.py | sort -r -g -k2 > output.txt; head output.txt

## Hadoop file system
bin/hdfs dfs -mv /tweets_2015-11-23.csv /input/tweets_2015-11-23.csv

## setup java
export JAVA_HOME=/usr/lib/jvm/default-java

## Run hadoop
bin/hdfs dfs -rm -r -f /output; bin/hadoop jar streaming.jar -mapper /home/dennis/Programs/hadoop/mapper.py -reducer /home/dennis/Programs/hadoop/reducer.py -input /input -output /output -file /home/dennis/Programs/hadoop/mapper.py -file /home/dennis/Programs/hadoop/reducer.py > output.txt 2>&1

## Create test files
head -n 10 tweets/tweets_2015-11-23.csv > tweets/test1.csv

##start file system with:
sbin/start-dfs.sh



On Mac:

## Do a run
cat tweets/test6.csv | venv/bin/python ./mapper.py | sort -k1,1 | venv/bin/python ./reducer.py | sort -r -g -k2 > output.txt; head output.txt

## Hadoop file system
hdfs dfs -mv /Users/Guido/Development/WebAnalyticsHadoop/tweets/new_years/tweets_2014-01-01_emo.csv /input/tweets_2014-01-01_emo.csv

## setup java
export JAVA_HOME=/usr/lib/jvm/default-java

## Run hadoop
hdfs dfs -rm -r -f /output; hadoop jar streaming.jar -mapper /Users/Guido/Dropbox/Studie/TUe/Year\ 3/2IID0\ -\ Web\ Analytics/Assignments/GitHub/web-analytics/mapper.py -reducer /Users/Guido/Dropbox/Studie/TUe/Year\ 3/2IID0\ -\ Web\ Analytics/Assignments/GitHub/web-analytics/reducer.py -input /input -output /output -file /Users/Guido/Dropbox/Studie/TUe/Year\ 3/2IID0\ -\ Web\ Analytics/Assignments/GitHub/web-analytics/mapper.py -file /Users/Guido/Dropbox/Studie/TUe/Year\ 3/2IID0\ -\ Web\ Analytics/Assignments/GitHub/web-analytics/reducer.py > output.txt 2>&1

## Create test files
head -n 10 tweets/tweets_2015-11-23.csv > tweets/test1.csv

##start file system with:
sbin/start-dfs.sh